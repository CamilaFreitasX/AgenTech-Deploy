import streamlit as st
import pandas as pd
import zipfile
import os
import tempfile
from io import StringIO

# Tente este import alternativo:
try:
    from langchain_experimental.agents import create_csv_agent
    from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent 
except ImportError:
    from langchain.agents import create_csv_agent
    # Para vers√µes mais antigas de langchain, create_pandas_dataframe_agent pode estar em langchain.agents
    try:
        from langchain.agents import create_pandas_dataframe_agent
    except ImportError:
        st.error("N√£o foi poss√≠vel importar 'create_pandas_dataframe_agent'. Verifique a instala√ß√£o do LangChain.")
        # Permitir que o app continue se este agente espec√≠fico n√£o for crucial ou usado.
        pass


from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.agents.agent_types import AgentType # AgentType ainda √© usado
from utils_google import NotaFiscalValidator, extract_zip_file # Supondo que utils_google.py exista e funcione
import warnings
from dotenv import load_dotenv

load_dotenv()
warnings.filterwarnings("ignore")

class CSVAnalysisAgent:
    def __init__(self, google_api_key=None):
        """Inicializa o agente de an√°lise CSV com Google Gemini"""
        self.google_api_key = google_api_key
        self.agents = {} 
        self.dataframes = {} 
        self.file_info = {} 
        self.cabecalho_file_type_name = "cabecalho" 

    def create_llm(self):
        """Cria uma inst√¢ncia do modelo Google Gemini"""
        if not self.google_api_key:
            self.google_api_key = os.getenv("GOOGLE_API_KEY")
            if not self.google_api_key:
                raise ValueError("Google API Key √© necess√°ria para usar o agente e n√£o foi encontrada.")
        
        try:
            llm = ChatGoogleGenerativeAI(
                model="gemini-1.5-flash",
                google_api_key=self.google_api_key,
                temperature=0.5,
                convert_system_message_to_human=True,
            )
            return llm
        except Exception as e:
            st.error(f"Erro ao criar modelo Gemini: {str(e)}")
            return None
    
    def load_csv_data(self, file_path, file_type):
        """Carrega dados CSV."""
        try:
            try:
                df = pd.read_csv(file_path, encoding='utf-8', sep=None, engine='python', on_bad_lines='warn')
            except UnicodeDecodeError:
                df = pd.read_csv(file_path, encoding='latin1', sep=None, engine='python', on_bad_lines='warn')
            except Exception as e_read: 
                st.error(f"Erro ao ler CSV {os.path.basename(file_path)}: {e_read}. Tentando com delimitador ';'...")
                try:
                    df = pd.read_csv(file_path, encoding='utf-8', sep=';', on_bad_lines='warn')
                except UnicodeDecodeError:
                    df = pd.read_csv(file_path, encoding='latin1', sep=';', on_bad_lines='warn')
                except Exception as e_read_semi:
                     st.error(f"N√£o foi poss√≠vel ler o arquivo {os.path.basename(file_path)} mesmo com sep=';': {e_read_semi}")
                     return False

            self.dataframes[file_type] = df
            self.file_info[file_type] = {
                'path': file_path, 
                'shape': df.shape,
                'columns': df.columns.tolist()
            }
            return True
            
        except Exception as e:
            st.error(f"Erro geral ao carregar arquivo {file_type} ({os.path.basename(file_path)}): {str(e)}")
            return False
    
    def create_general_agent(self):
        """Cria um agente geral que pode acessar todos os dataframes carregados."""
        try:
            if not self.dataframes:
                print("Erro: Nenhum dataframe carregado.")
                st.error("Nenhum dataframe carregado para criar o agente geral.")
                return None
            
            llm = self.create_llm()
            if llm is None:
                print("Erro: N√£o foi poss√≠vel criar LLM para o agente geral.")
                st.error("N√£o foi poss√≠vel criar o modelo de linguagem para o agente geral.")
                return None
            
            ordered_df_tuples = []
            for file_type_key in self.dataframes.keys(): 
                df_instance = self.dataframes.get(file_type_key)
                if isinstance(df_instance, pd.DataFrame):
                     ordered_df_tuples.append((file_type_key, df_instance))
                else:
                    print(f"Aviso: '{file_type_key}' n√£o √© um DataFrame v√°lido. Ser√° ignorado.")

            if not ordered_df_tuples:
                print("Erro: Nenhum dataframe v√°lido para o agente.")
                st.error("Nenhum dataframe v√°lido encontrado para criar o agente.")
                return None

            list_of_dfs = [df_tuple[1] for df_tuple in ordered_df_tuples]
            list_of_df_names = [df_tuple[0] for df_tuple in ordered_df_tuples]
            
            cabecalho_df_index = -1
            st.session_state.pop('cabecalho_df_variable_name', None) 
            try:
                cabecalho_df_index = list_of_df_names.index(self.cabecalho_file_type_name)
                st.session_state.cabecalho_df_variable_name = f"df_{cabecalho_df_index}"
                print(f"√çndice do DataFrame de cabe√ßalho ('{self.cabecalho_file_type_name}'): {cabecalho_df_index} (ser√° {st.session_state.cabecalho_df_variable_name})")
            except ValueError:
                print(f"Aviso: DataFrame do tipo '{self.cabecalho_file_type_name}' n√£o encontrado na lista: {list_of_df_names}")

            # Estrat√©gia Principal: create_pandas_dataframe_agent se houver DataFrames
            if list_of_dfs:
                try:
                    print(f"üîÑ Tentando criar agente pandas com {len(list_of_dfs)} dataframes: {list_of_df_names}")
                    
                    prefix_parts = [
                        "Voc√™ √© um agente de an√°lise de dados altamente competente, especializado em notas fiscais brasileiras.",
                        f"Voc√™ tem acesso a {len(list_of_dfs)} dataframes pandas nomeados df_0, df_1, ...:",
                    ]
                    for i, name in enumerate(list_of_df_names):
                        cols_list = self.dataframes[name].columns.to_list() if isinstance(self.dataframes.get(name), pd.DataFrame) else ["Colunas n√£o dispon√≠veis"]
                        prefix_parts.append(f"- df_{i}: (Tipo Original: '{name}'). Colunas: {cols_list}")

                    if cabecalho_df_index != -1:
                        prefix_parts.append(f"IMPORTANTE: O dataframe 'df_{cabecalho_df_index}' (tipo '{self.cabecalho_file_type_name}') √© o principal para an√°lise de totais por fornecedor, pois cont√©m os cabe√ßalhos das notas fiscais. Use-o para essa finalidade.")
                    else:
                        prefix_parts.append(f"IMPORTANTE: O dataframe do tipo '{self.cabecalho_file_type_name}' √© o principal para an√°lise de totais por fornecedor. Identifique qual df_X (df_0, df_1, etc.) corresponde a este tipo e use-o para essa finalidade.")
                    
                    prefix_message = "\n".join(prefix_parts)
                    input_for_agent = list_of_dfs[0] if len(list_of_dfs) == 1 else list_of_dfs

                    general_agent = create_pandas_dataframe_agent(
                        llm,
                        input_for_agent, 
                        verbose=True,
                        agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
                        allow_dangerous_code=True,
                        prefix=prefix_message,
                        # Args do AgentExecutor passados diretamente:
                        handle_parsing_errors=" –∞–≥–µ–Ω—Ç–æ–º –±—É–¥–µ—Ç –ø—Ä–µ–¥–ø—Ä–∏–Ω—è—Ç–∞ –ø–æ–ø—ã—Ç–∫–∞ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –æ—à–∏–±–∫—É —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ ",
                        max_iterations=15,
                        early_stopping_method="generate",
                        max_execution_time=600 
                    )
                    print(f"‚úÖ Agente pandas criado com sucesso para {len(list_of_dfs)} dataframes.")
                    st.session_state.current_agent_type = "pandas_multi"
                    return general_agent
                
                except Exception as e_pandas:
                    print(f"‚ùå Erro ao criar agente pandas: {str(e_pandas)}")
                    st.warning(f"Falha ao criar agente pandas: {e_pandas}. Tentando fallback para agente CSV (se houver caminhos v√°lidos)...")
                    st.session_state.pop('cabecalho_df_variable_name', None) 

            # Fallback para create_csv_agent (usando caminhos)
            valid_paths = [self.file_info.get(ft, {}).get('path') for ft_name in list_of_df_names 
                           if self.file_info.get(ft_name, {}).get('path') and os.path.exists(self.file_info.get(ft_name, {}).get('path'))]


            if not valid_paths:
                st.error("Nenhum caminho de arquivo CSV v√°lido encontrado para o agente de fallback.")
                return None
            
            agent_path_input = valid_paths[0] if len(valid_paths) == 1 else valid_paths
            print(f"Tentando criar agente CSV com {len(valid_paths)} arquivos: {agent_path_input}")
            try:
                general_agent = create_csv_agent(
                    llm,
                    agent_path_input, 
                    verbose=True,
                    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
                    allow_dangerous_code=True,
                    # Args do AgentExecutor passados diretamente:
                    handle_parsing_errors=True, 
                    max_iterations=30,
                    early_stopping_method="generate",
                    max_execution_time=600 
                )
                print(f"‚úÖ Agente CSV (path-based) criado com sucesso para {len(valid_paths)} arquivos.")
                st.session_state.current_agent_type = "csv_multi_path" if isinstance(agent_path_input, list) else "csv_single_path"
                
                if st.session_state.current_agent_type == "csv_single_path":
                    first_file_type_path_fallback = None # Renomeada para evitar conflito de escopo
                    for f_type_fallback, info_fallback in self.file_info.items():
                        if info_fallback['path'] == agent_path_input: 
                            first_file_type_path_fallback = f_type_fallback
                            break
                    if first_file_type_path_fallback == self.cabecalho_file_type_name:
                        st.session_state.cabecalho_df_variable_name = "df" 
                    else:
                        st.session_state.pop('cabecalho_df_variable_name', None)
                return general_agent
            except Exception as e_csv:
                print(f"‚ùå Erro ao criar agente CSV (path-based): {str(e_csv)}")
                st.error(f"Falha cr√≠tica ao criar qualquer tipo de agente: {e_csv}") 
                return None

        except Exception as e_general:
            print(f"‚ùå Erro geral n√£o capturado anteriormente ao criar agente: {str(e_general)}")
            st.error(f"Erro geral e fatal ao criar agente de an√°lise: {str(e_general)}")
            return None

    def query(self, question):
        """Executa uma consulta usando o agente geral."""
        try:
            agent = self.create_general_agent() 
            if agent is None:
                return "Erro: N√£o foi poss√≠vel inicializar o agente de an√°lise para a consulta."

            target_df_variable_for_prompt = "df" 
            identified_cabecalho_type_for_prompt = f"'{self.cabecalho_file_type_name}'"
            
            current_agent_context = st.session_state.get("current_agent_type")
            cabecalho_df_var_from_session = st.session_state.get("cabecalho_df_variable_name")

            if current_agent_context == "pandas_multi" and cabecalho_df_var_from_session:
                target_df_variable_for_prompt = cabecalho_df_var_from_session
                print(f"Query: Usando '{target_df_variable_for_prompt}' para c√≥digo pandas (agente pandas_multi).")
            elif current_agent_context in ["csv_single_path", "csv_single_fallback"] and cabecalho_df_var_from_session == "df": # "csv_single_fallback" foi removido antes
                target_df_variable_for_prompt = "df"
                print(f"Query: Usando 'df' para c√≥digo pandas (agente CSV √∫nico sobre cabe√ßalho).")
            elif current_agent_context == "csv_multi_path":
                print(f"Query: Agente csv_multi_path. O agente precisar√° identificar e carregar o CSV '{self.cabecalho_file_type_name}'.")
                identified_cabecalho_type_for_prompt = f"o arquivo CSV correspondente a '{self.cabecalho_file_type_name}'"
            else: 
                 print(f"AVISO: Contexto do agente √© '{current_agent_context}'. O nome da vari√°vel do dataframe de cabe√ßalho n√£o foi determinado explicitamente para o prompt. O Agente precisar√° inferir qual dataframe usar para '{self.cabecalho_file_type_name}'.")
                 identified_cabecalho_type_for_prompt = f"o dataframe/arquivo correspondente a '{self.cabecalho_file_type_name}'"


            coluna_fornecedor = 'RAZ√ÉO SOCIAL EMITENTE'
            coluna_valor = 'VALOR NOTA FISCAL'

            pandas_code_block = f"""
```python
import pandas as pd

# O agente deve usar o dataframe que representa os dados de {identified_cabecalho_type_for_prompt}.
# No contexto de um agente pandas_multi, esta vari√°vel j√° foi definida (ex: df_0, df_1), use '{target_df_variable_for_prompt}'.
# No contexto de um agente CSV (path-based), carregue o arquivo CSV de cabe√ßalho em um dataframe (ex: {target_df_variable_for_prompt} = pd.read_csv('path_do_cabecalho.csv')).
# O c√≥digo abaixo assume que '{target_df_variable_for_prompt}' √© o dataframe de cabe√ßalho j√° dispon√≠vel ou carregado.

print(f"Iniciando an√°lise no dataframe '{target_df_variable_for_prompt}' (espera-se que seja {identified_cabecalho_type_for_prompt}).")
try:
    # Esta parte √© crucial: o agente deve assegurar que '{target_df_variable_for_prompt}' √© o dataframe correto.
    # Se o agente precisar carregar um CSV, ele deve fazer isso aqui.
    # Exemplo de l√≥gica que o agente pode precisar executar internamente (n√£o para copiar literalmente):
    # if not isinstance({target_df_variable_for_prompt}, pd.DataFrame) and isinstance({target_df_variable_for_prompt}, str) and ".csv" in {target_df_variable_for_prompt}:
    #     # Supondo que target_df_variable_for_prompt cont√©m o caminho para o CSV de cabe√ßalho
    #     actual_df_for_analysis = pd.read_csv({target_df_variable_for_prompt}) 
    # else:
    #     actual_df_for_analysis = {target_df_variable_for_prompt} # J√° √© um DataFrame

    # Para o c√≥digo abaixo, vamos assumir que 'actual_df_for_analysis' √© o nome do dataframe que o agente usar√°.
    # Por simplicidade no template, continuaremos usando '{target_df_variable_for_prompt}', mas o agente deve entender este mapeamento.

    print("Colunas dispon√≠veis em '{target_df_variable_for_prompt}':", {target_df_variable_for_prompt}.columns.tolist())
    print("\\nPrimeiras 5 linhas de '{target_df_variable_for_prompt}':\\n", {target_df_variable_for_prompt}.head())

    if '{coluna_valor}' not in {target_df_variable_for_prompt}.columns:
        raise ValueError("Coluna de valor '{coluna_valor}' N√ÉO ENCONTRADA no dataframe '{target_df_variable_for_prompt}'. Verifique o nome da coluna e o dataframe usado.")
    if '{coluna_fornecedor}' not in {target_df_variable_for_prompt}.columns:
        raise ValueError("Coluna de fornecedor '{coluna_fornecedor}' N√ÉO ENCONTRADA no dataframe '{target_df_variable_for_prompt}'. Verifique o nome da coluna e o dataframe usado.")

    df_analysis = {target_df_variable_for_prompt}.copy() 
    if df_analysis['{coluna_valor}'].dtype == 'object':
        df_analysis.loc[:, '{coluna_valor}'] = df_analysis['{coluna_valor}'].astype(str).str.replace('.', '', regex=False).str.replace(',', '.', regex=False)
    
    df_analysis.loc[:, '{coluna_valor}'] = pd.to_numeric(df_analysis['{coluna_valor}'], errors='coerce')
    
    print(f"Valores ausentes em '{coluna_valor}' ap√≥s convers√£o num√©rica: {{df_analysis['{coluna_valor}'].isnull().sum()}}")
    df_analysis.dropna(subset=['{coluna_valor}', '{coluna_fornecedor}'], inplace=True) 

    if df_analysis.empty:
        raise ValueError("O DataFrame ficou vazio ap√≥s limpeza de dados (NaN em valor ou fornecedor). N√£o √© poss√≠vel prosseguir.")

    print(f"Agrupando por '{coluna_fornecedor}' e somando '{coluna_valor}'. Total de linhas para agrupar: {{len(df_analysis)}}")
    resultado = df_analysis.groupby('{coluna_fornecedor}')['{coluna_valor}'].sum()

    resultado_ordenado = resultado.sort_values(ascending=False)

    if not resultado_ordenado.empty:
        maior_fornecedor = resultado_ordenado.index[0]
        maior_valor = resultado_ordenado.iloc[0]
        
        # Formata√ß√£o para o padr√£o monet√°rio brasileiro R$ xxx.xxx,xx
        # Usando separador de milhar '.' e decimal ','
        maior_valor_formatado = f"R$ {{:_.2f}}".format(maior_valor).replace('.', '#TEMP#').replace(',', '.').replace('#TEMP#', ',').replace('_', '.')
        # Corre√ß√£o: O de cima pode n√£o funcionar bem com o _ para .
        # Melhor: formatar para string, depois substituir.
        # Ex: 1292418.75 -> "1.292.418,75"
        s_valor = f"{{maior_valor:.2f}}" # "1292418.75"
        partes = s_valor.split('.')
        inteiro = partes[0]
        decimal = partes[1] if len(partes) > 1 else "00"
        inteiro_formatado = ""
        if len(inteiro) > 3:
            for i, digito in enumerate(reversed(inteiro)):
                if i > 0 and i % 3 == 0:
                    inteiro_formatado = "." + inteiro_formatado
                inteiro_formatado = digito + inteiro_formatado
        else:
            inteiro_formatado = inteiro
        maior_valor_formatado = f"R$ {{inteiro_formatado}},{{decimal}}"


        print(f"\\nFORNECEDOR COM MAIOR MONTANTE TOTAL CONSOLIDADO: {{maior_fornecedor}}")
        print(f"VALOR TOTAL CONSOLIDADO: {{maior_valor_formatado}}")

        print("\\nTOP 5 FORNECEDORES (VALORES TOTAIS CONSOLIDADOS):")
        for i, (fornecedor_item, valor_item) in enumerate(resultado_ordenado.head().items()):
            s_valor_item = f"{{valor_item:.2f}}"
            partes_item = s_valor_item.split('.')
            inteiro_item = partes_item[0]
            decimal_item = partes_item[1] if len(partes_item) > 1 else "00"
            inteiro_fmt_item = ""
            if len(inteiro_item) > 3:
                 for j, digito_item in enumerate(reversed(inteiro_item)):
                    if j > 0 and j % 3 == 0:
                        inteiro_fmt_item = "." + inteiro_fmt_item
                    inteiro_fmt_item = digito_item + inteiro_fmt_item
            else:
                inteiro_fmt_item = inteiro_item
            valor_fmt_item = f"R$ {{inteiro_fmt_item}},{{decimal_item}}"
            print(f"{{i+1}}. {{fornecedor_item}}: {{valor_fmt_item}}")
    else:
        print("N√£o foi poss√≠vel calcular os resultados. O dataframe agrupado est√° vazio (nenhum fornecedor encontrado ou todos os valores eram inv√°lidos).")
        
except Exception as e_pandas_code:
    print(f"ERRO AO EXECUTAR O C√ìDIGO PANDAS DENTRO DO AGENTE: {{str(e_pandas_code)}}")
    print("Verifique se o dataframe '{target_df_variable_for_prompt}' foi carregado/identificado corretamente pelo agente e se as colunas '{coluna_fornecedor}' e '{coluna_valor}' existem e s√£o adequadas para a an√°lise.")

```"""
            available_data_message = f"DADOS DISPON√çVEIS NO SISTEMA: {', '.join(self.dataframes.keys()) if self.dataframes else 'Nenhum dataframe carregado diretamente no sistema Python. Agente pode precisar carregar de caminhos.'}"
            
            guidance_on_df_usage = (
                f"Voc√™ DEVE USAR o dataframe '{target_df_variable_for_prompt}' (que o sistema identificou como correspondendo a {identified_cabecalho_type_for_prompt}) para esta an√°lise, pois ele cont√©m os cabe√ßalhos das notas fiscais."
                if cabecalho_df_var_from_session or (current_agent_context in ["csv_single_path"] and identified_cabecalho_type_for_prompt == f"'{self.cabecalho_file_type_name}'") # csv_single_fallback removido
                else f"Voc√™ deve priorizar o uso dos dados do arquivo/dataframe do tipo '{self.cabecalho_file_type_name}'. Se m√∫ltiplos dataframes (df_0, df_1, ...) ou arquivos CSV estiverem dispon√≠veis, identifique qual deles corresponde a '{self.cabecalho_file_type_name}' (o prefixo do agente, se aplic√°vel, j√° deu essa informa√ß√£o) e use-o para a an√°lise de totais por fornecedor. O c√≥digo pandas fornecido usa '{target_df_variable_for_prompt}' como placeholder para este dataframe de cabe√ßalho."
            )

            enhanced_question = f"""
OBJETIVO PRINCIPAL: Identificar o fornecedor com o MAIOR VALOR TOTAL CONSOLIDADO de notas fiscais.

{available_data_message}
{guidance_on_df_usage}

PERGUNTA ORIGINAL DO USU√ÅRIO: {question}

‚ö†Ô∏è ATEN√á√ÉO CR√çTICA E OBRIGAT√ìRIA - LEIA COM ATEN√á√ÉO M√ÅXIMA:
Para determinar o "maior" fornecedor, N√ÉO olhe para valores de notas fiscais individuais.
VOC√ä DEVE, OBRIGATORIAMENTE, SEGUIR ESTES PASSOS:
1. AGRUPAR OS DADOS POR FORNECEDOR (usando a coluna '{coluna_fornecedor}').
2. SOMAR (usar a fun√ß√£o `.sum()`) TODOS os valores de notas fiscais (da coluna '{coluna_valor}') para CADA fornecedor individualmente.
Isto resultar√° no MONTANTE TOTAL CONSOLIDADO para cada fornecedor. A an√°lise √© sobre este valor consolidado.

METODOLOGIA OBRIGAT√ìRIA PARA RESPONDER CORRETAMENTE:
1. IDENTIFICA√á√ÉO DO DATAFRAME: Certifique-se de que est√° usando o DataFrame correto que cont√©m os dados de cabe√ßalho das notas fiscais (referido como '{target_df_variable_for_prompt}' no c√≥digo abaixo, que deve corresponder a {identified_cabecalho_type_for_prompt}). Se for um agente baseado em CSV, carregue o arquivo CSV de cabe√ßalho apropriado.
2. COLUNAS CRUCIAIS: As colunas s√£o '{coluna_fornecedor}' (nome do fornecedor) e '{coluna_valor}' (valor da nota).
3. CONVERS√ÉO DE VALOR (ESSENCIAL!): ANTES DE QUALQUER C√ÅLCULO, a coluna '{coluna_valor}' DEVE ser convertida para um tipo num√©rico (float). √â comum ela estar como string (ex: "1.234,56" ou "6712.16"). O c√≥digo pandas fornecido abaixo j√° inclui a l√≥gica para converter strings no formato "X.XXX,YY" para o num√©rico XXXX.YY (removendo '.' e trocando ',' por '.'). Valores que n√£o puderem ser convertidos devem se tornar NaN e as linhas correspondentes (ou pelo menos com valor NaN) devem ser removidas antes da agrega√ß√£o.
4. AGRUPAMENTO: Agrupe os dados pela coluna '{coluna_fornecedor}'.
5. SOMA CONSOLIDADA: SOME (use `.sum()`) TODOS os valores da coluna '{coluna_valor}' para CADA fornecedor.
6. ORDENA√á√ÉO: Ordene os fornecedores de forma descendente pelo valor total CONSOLIDADO.
7. RESULTADO: O fornecedor no topo desta lista ordenada √© a resposta.

‚ùå EXEMPLO CLAR√çSSIMO DO ERRO GRAVE A SER EVITADO (N√ÉO FA√áA ISSO DE FORMA ALGUMA):
- ERRADO: Olhar uma √∫nica nota fiscal da "EDITORA FTD S.A." no valor de R$ 6.712,16 e, por ser um valor alto individual, concluir que este √© o resultado. ISTO EST√Å COMPLETAMENTE ERRADO.
- CORRETO: Somar TODAS as notas fiscais da "EDITORA FTD S.A." (ex: o total consolidado dela pode ser R$ 292.486,11) e comparar esta SOMA CONSOLIDADA com a SOMA CONSOLIDADA de TODAS as notas de outros fornecedores.
- EXEMPLO DE RESULTADO CORRETO ESPERADO (valores meramente ilustrativos para o conceito): "O fornecedor CHEMYUNION LTDA teve o maior montante total consolidado, com R$ 1.292.418,75."

‚û°Ô∏è C√ìDIGO PANDAS OBRIGAT√ìRIO - Execute a l√≥gica descrita abaixo. Adapte '{target_df_variable_for_prompt}' para o nome correto do dataframe de cabe√ßalho se ele for diferente (ex: df_0, df_1, ou o resultado de pd.read_csv('caminho/para/cabecalho.csv')). O agente √© respons√°vel por garantir que '{target_df_variable_for_prompt}' no c√≥digo abaixo seja o dataframe correto de cabe√ßalho.
{pandas_code_block}

FORMATO DA RESPOSTA FINAL (em portugu√™s brasileiro):
- "O fornecedor com o maior montante total consolidado √©: [Nome do Fornecedor]."
- "Valor total consolidado: [Valor Formatado no padr√£o R$ xxx.xxx,xx]."
- Inclua uma se√ß√£o "C√≥digo Pandas Chave Executado e Observa√ß√µes:" onde voc√™ mostra o trecho principal do c√≥digo pandas que voc√™ efetivamente usou para o c√°lculo (ou um resumo claro da sua execu√ß√£o, qual dataframe foi usado, e a forma como '{coluna_valor}' foi tratada e agregada) e quaisquer observa√ß√µes importantes (ex: n√∫mero de linhas analisadas, tratamento de erros).
- "Top 5 fornecedores por valor total consolidado:"
  1. [Fornecedor A]: [Valor A Formatado R$ xxx.xxx,xx]
  2. [Fornecedor B]: [Valor B Formatado R$ xxx.xxx,xx]
  ... e assim por diante.

INSTRU√á√ïES CR√çTICAS ADICIONAIS (RELEIA ANTES DE RESPONDER):
1. FOCO ABSOLUTO NA AGREGA√á√ÉO: `groupby('{coluna_fornecedor}')['{coluna_valor}'].sum()`. √â a chave.
2. VALIDA√á√ÉO DE DADOS: SEMPRE verifique `df.info()`, `df.head()` do dataframe de cabe√ßalho. A convers√£o da coluna de valor √© a etapa mais cr√≠tica antes da soma.
3. FORMATA√á√ÉO MONET√ÅRIA BRASILEIRA: Apresente valores finais como R$ XX.XXX.XXX,XX (ponto como separador de milhar, v√≠rgula para decimal). O c√≥digo exemplo tenta fazer isso. Certifique-se que a sa√≠da esteja correta.
4. RESPONDA EM PORTUGU√äS BRASILEIRO.
5. N√ÉO INVENTE DADOS. Use apenas os dados dos arquivos fornecidos.
6. SEJA EXPL√çCITO sobre qual arquivo/dataframe foi usado para a an√°lise principal e como chegou ao resultado.
7. SE O DATAFRAME DE CABE√áALHO ESTIVER VAZIO ou n√£o contiver as colunas necess√°rias, informe isso claramente.
"""
            
            response = agent.invoke({"input": enhanced_question})
            
            result = ""
            if isinstance(response, dict):
                result = response.get("output", response.get("result", response.get("answer", "")))
                if not result and response: 
                    for val in response.values():
                        if isinstance(val, str):
                            result = val
                            break
            else:
                result = str(response)
            
            if result and result.strip():
                result_str = result.strip()
                lines = result_str.split('\n')
                clean_lines = []
                in_tool_code_block = False 
                for line in lines:
                    if line.strip().startswith("```python"):
                        in_tool_code_block = True
                        clean_lines.append(line)
                        continue
                    if line.strip().startswith("```") and in_tool_code_block:
                        in_tool_code_block = False
                        clean_lines.append(line)
                        continue
                    
                    if in_tool_code_block: 
                         clean_lines.append(line)
                         continue

                    debug_terms_to_remove = [
                        '> entering new agentexecutor chain', 
                        '> entering new llmchain object',
                        'invoking agent with', 'invoking llm', 
                        'tool execution result', 'action:', 'action input:', 'observation:',
                    ]
                    temp_line_lower = line.lower()
                    # Evitar remover linhas que contenham "thought:" mas s√£o parte da resposta √∫til,
                    # a menos que sejam apenas "Thought: ..." no in√≠cio de uma linha de log.
                    # A heur√≠stica aqui √© se a linha *come√ßa* com "thought:" (ignorando espa√ßos)
                    # e √© prov√°vel que seja um log.
                    is_log_line = False
                    if temp_line_lower.lstrip().startswith("thought:"):
                        is_log_line = True
                    
                    for term in debug_terms_to_remove:
                        if temp_line_lower.startswith(term):
                            is_log_line = True
                            break
                    
                    if not is_log_line:
                        if temp_line_lower.strip() not in ["okay.", "got it.", "sure."]:
                            # Remover "Final Answer:" se estiver no in√≠cio da linha
                            if line.lstrip().startswith("Final Answer:"):
                                clean_lines.append(line.lstrip()[len("Final Answer:"):].lstrip())
                            elif line.lstrip().startswith("FINAL ANSWER:"):
                                clean_lines.append(line.lstrip()[len("FINAL ANSWER:"):].lstrip())
                            else:
                                clean_lines.append(line)

                result_str = '\n'.join(clean_lines).strip()

                result_str = result_str.replace('The supplier with the highest total consolidated amount is:', 'O fornecedor com o maior montante total consolidado √©:')
                result_str = result_str.replace('Total consolidated value:', 'Valor total consolidado:')
                result_str = result_str.replace('Top 5 suppliers by total consolidated value:', 'Top 5 fornecedores por valor total consolidado:')
                result_str = result_str.replace('Key Pandas Code Executed and Observations:', 'C√≥digo Pandas Chave Executado e Observa√ß√µes:')

                return result_str
            else:
                return "O agente n√£o conseguiu processar a consulta ou n√£o retornou uma resposta estruturada. Verifique os logs do console para depura√ß√£o."
                
        except Exception as e:
            st.error(f"Erro cr√≠tico ao processar consulta na camada de query: {str(e)}")
            import traceback
            traceback.print_exc() 
            return f"Erro cr√≠tico ao processar sua consulta: {str(e)}. Por favor, verifique os logs do console do servidor ou tente novamente."

    def get_data_summary(self):
        """Retorna um resumo dos dados carregados."""
        summary = {}
        if not self.dataframes:
            return {"message": "Nenhum dado carregado ainda."}
        for file_type, df_instance in self.dataframes.items():
            if isinstance(df_instance, pd.DataFrame):
                summary[file_type] = {
                    'linhas': len(df_instance),
                    'colunas': len(df_instance.columns),
                    'colunas_lista': df_instance.columns.tolist(),
                    'tipos': {col: str(dtype) for col, dtype in df_instance.dtypes.to_dict().items()},
                    'primeiras_linhas': df_instance.head().to_dict('records')
                }
            else:
                summary[file_type] = {"error": f"'{file_type}' n√£o √© um DataFrame v√°lido ou n√£o foi carregado."}
        return summary

def main():
    st.set_page_config(
        page_title="Agente de An√°lise de Notas Fiscais - Google Gemini",
        page_icon="üìä",
        layout="wide"
    )
    
    st.title("ü§ñ Agente Inteligente para An√°lise de Notas Fiscais")
    st.markdown("### Utilizando Google Gemini API & LangChain")
    st.markdown("---")
    
    if 'csv_agent_instance' not in st.session_state:
        try:
            # A API Key ser√° lida de .env dentro de create_llm se n√£o fornecida aqui
            st.session_state.csv_agent_instance = CSVAnalysisAgent() 
        except ValueError as e: 
            st.error(f"Erro de inicializa√ß√£o do agente: {e}")
            return
        except Exception as e_init: # Outras exce√ß√µes na inicializa√ß√£o
            st.error(f"Erro inesperado na inicializa√ß√£o do CSVAnalysisAgent: {e_init}")
            return

    csv_analyzer = st.session_state.csv_agent_instance

    st.header("üìÅ 1. Upload do Arquivo ZIP com CSVs")
    uploaded_file = st.file_uploader(
        "Fa√ßa upload do arquivo ZIP contendo os CSVs de notas fiscais (ex: cabecalho.csv, itens.csv)",
        type=['zip'],
        help=f"O ZIP deve conter arquivos CSV. Um deles deve ser o de '{csv_analyzer.cabecalho_file_type_name}' das notas fiscais para a an√°lise principal."
    )
    
    if uploaded_file is not None:
        with st.spinner("Processando arquivo ZIP e carregando dados..."):
            csv_analyzer.dataframes = {}
            csv_analyzer.file_info = {}
            st.session_state.pop('cabecalho_df_variable_name', None) 
            st.session_state.pop('current_agent_type', None)

            temp_dir_path = None # Para garantir que tem um valor
            try:
                # Assegurar que utils_google.py e suas fun√ß√µes est√£o dispon√≠veis e corretas.
                # Se extract_zip_file ou NotaFiscalValidator n√£o estiverem definidos ou importados,
                # o c√≥digo falhar√° aqui.
                if 'extract_zip_file' not in globals() or 'NotaFiscalValidator' not in globals():
                     st.error("Fun√ß√µes utilit√°rias 'extract_zip_file' ou 'NotaFiscalValidator' n√£o encontradas. Verifique 'utils_google.py'.")
                     return # Parar execu√ß√£o se utilit√°rios n√£o estiverem l√°

                temp_dir_path = extract_zip_file(uploaded_file) 
            except NameError as ne:
                st.error(f"Erro de nome: {ne}. A fun√ß√£o 'extract_zip_file' ou 'NotaFiscalValidator' n√£o foi definida ou importada corretamente de 'utils_google.py'.")
                return
            except Exception as e_zip:
                st.error(f"Falha ao extrair o arquivo ZIP: {e_zip}")
                temp_dir_path = None # Atribui√ß√£o expl√≠cita em caso de falha
            
            if temp_dir_path: # Procede apenas se temp_dir_path for um caminho v√°lido
                st.success(f"‚úÖ Arquivo ZIP extra√≠do com sucesso para o diret√≥rio tempor√°rio.")
                
                try:
                    validator = NotaFiscalValidator() 
                except Exception as e_val_init:
                     st.error(f"Erro ao inicializar NotaFiscalValidator: {e_val_init}. Assegure-se que 'utils_google.py' est√° correto.")
                     return # Parar se o validador falhar

                csv_files_found = [f for f in os.listdir(temp_dir_path) if f.lower().endswith('.csv')]
                
                if not csv_files_found:
                    st.warning("Nenhum arquivo CSV encontrado no ZIP.")
                else:
                    loaded_files_summary = []
                    for csv_file_name in csv_files_found:
                        file_full_path = os.path.join(temp_dir_path, csv_file_name)
                        try:
                            file_type_identified = validator.identify_file_type(file_full_path) 
                        except Exception as e_ident:
                            st.warning(f"Erro ao identificar tipo do arquivo {csv_file_name} com NotaFiscalValidator: {e_ident}. Usando nome do arquivo como tipo.")
                            file_type_identified = os.path.splitext(csv_file_name)[0].lower().replace(" ", "_")


                        if file_type_identified == "unknown": 
                            file_type_identified = f"tipo_desconhecido_{os.path.splitext(csv_file_name)[0].replace(' ', '_')}"

                        success = csv_analyzer.load_csv_data(file_full_path, file_type_identified)
                        if success:
                            loaded_files_summary.append(f"{csv_file_name} (tipo: '{file_type_identified}')")
                
                    if loaded_files_summary:
                        st.success(f"‚úÖ Arquivos CSV carregados: {', '.join(loaded_files_summary)}")
                        
                        if csv_analyzer.cabecalho_file_type_name not in csv_analyzer.dataframes:
                            st.error(f"‚ÄºÔ∏è ATEN√á√ÉO: O arquivo/tipo de dados '{csv_analyzer.cabecalho_file_type_name}' √© ESSENCIAL para a an√°lise principal de totais por fornecedor e N√ÉO FOI ENCONTRADO ou identificado corretamente. Verifique o conte√∫do do ZIP e a l√≥gica de identifica√ß√£o em 'utils_google.py'. A an√°lise principal pode falhar.")
                        else:
                            st.info(f"üëç Arquivo/tipo de dados '{csv_analyzer.cabecalho_file_type_name}' carregado. Pronto para an√°lise de totais por fornecedor.")

                        with st.expander("üìä Resumo dos Dados Carregados"):
                            summary = csv_analyzer.get_data_summary()
                            for file_type_sum, info_sum in summary.items():
                                st.subheader(f"üìÑ {file_type_sum.replace('_', ' ').title()}")
                                if "error" in info_sum:
                                    st.error(info_sum["error"])
                                    continue
                                col1, col2 = st.columns(2)
                                with col1:
                                    st.metric("Linhas", info_sum.get('linhas', 'N/A'))
                                with col2:
                                    st.metric("Colunas", info_sum.get('colunas', 'N/A'))
                                st.write("**Nomes das Colunas:**")
                                st.caption(", ".join(info_sum.get('colunas_lista', [])))
                                
                                if st.checkbox(f"Ver primeiras 5 linhas - {file_type_sum}", key=f"show_head_{file_type_sum}"):
                                    df_preview = pd.DataFrame(info_sum.get('primeiras_linhas', []))
                                    st.dataframe(df_preview, use_container_width=True)
                    else:
                        st.error("Nenhum arquivo CSV p√¥de ser carregado do ZIP ou todos falharam.")
            # Limpar uploaded_file para permitir novo upload do mesmo arquivo (se desejado)
            st.session_state.uploaded_file_state = None 


    if hasattr(csv_analyzer, 'dataframes') and csv_analyzer.dataframes:
        st.header("üîç 2. Fa√ßa sua Pergunta ao Agente")
        
        with st.expander("üí° Exemplos de Perguntas (foco na an√°lise de totais consolidados do 'cabecalho')"):
            st.markdown(f"""
            - Qual √© o fornecedor que teve maior montante recebido (total consolidado)? (Pergunta principal)
            - Quais s√£o os 5 fornecedores com maior valor total consolidado?
            - Qual a soma total de '{csv_analyzer.dataframes[csv_analyzer.cabecalho_file_type_name].columns[csv_analyzer.dataframes[csv_analyzer.cabecalho_file_type_name].columns.str.contains('VALOR', case=False)][0] if csv_analyzer.cabecalho_file_type_name in csv_analyzer.dataframes and any(csv_analyzer.dataframes[csv_analyzer.cabecalho_file_type_name].columns.str.contains('VALOR', case=False)) else 'VALOR NOTA FISCAL'}' no arquivo '{csv_analyzer.cabecalho_file_type_name}'?
            - Quantas notas fiscais (linhas) existem no arquivo '{csv_analyzer.cabecalho_file_type_name}'?
            - Descreva as colunas do arquivo '{csv_analyzer.cabecalho_file_type_name}'.
            """)
        
        default_question = "Qual √© o fornecedor que teve maior montante recebido (total consolidado)?"
        if 'user_question_input_memory' not in st.session_state:
            st.session_state.user_question_input_memory = default_question

        user_question = st.text_area(
            "Sua pergunta:",
            value=st.session_state.user_question_input_memory,
            height=100,
            key="user_question_input"
        )
        st.session_state.user_question_input_memory = user_question 
        
        if st.button("üöÄ Executar Consulta", type="primary", key="run_query_button"):
            if not user_question.strip():
                st.warning("‚ö†Ô∏è Por favor, digite uma pergunta.")
            elif csv_analyzer.cabecalho_file_type_name not in csv_analyzer.dataframes and "total" in user_question.lower() and "fornecedor" in user_question.lower() : # Checagem mais espec√≠fica
                 st.error(f"‚ÄºÔ∏è N√£o √© poss√≠vel executar a consulta principal de totais por fornecedor. O arquivo/tipo de dados '{csv_analyzer.cabecalho_file_type_name}' n√£o foi carregado. Fa√ßa o upload de um ZIP contendo-o.")
            else:
                with st.spinner("üß† O Agente Gemini est√° processando sua consulta... Por favor, aguarde."):
                    response_from_agent = csv_analyzer.query(user_question) 
                    
                    st.markdown("### üìã Resposta do Agente:")
                    if response_from_agent and isinstance(response_from_agent, str) and response_from_agent.strip():
                        st.markdown(response_from_agent)
                    elif not response_from_agent:
                         st.error("O agente n√£o retornou uma resposta ou a resposta foi vazia.")
                    else: 
                        st.error(f"Resposta inesperada do agente (tipo: {type(response_from_agent)}): {response_from_agent}")
                    
                    if st.session_state.get("current_agent_type"):
                        st.caption(f"Debug Info: Agente usado: {st.session_state.current_agent_type}, Var. Cabe√ßalho no prompt: {st.session_state.get('cabecalho_df_variable_name', 'N/A')}")

    else:
        st.info("Fa√ßa o upload de um arquivo ZIP na se√ß√£o acima para come√ßar a an√°lise.")

    st.sidebar.markdown("---")
    st.sidebar.markdown("### üìö Sobre a Aplica√ß√£o")
    st.sidebar.markdown("""
    Esta aplica√ß√£o demonstra o uso de Intelig√™ncia Artificial Generativa para analisar dados de notas fiscais em formato CSV.
    - **IA:** Google Gemini API (Modelo gemini-1.5-flash)
    - **Orquestra√ß√£o:** LangChain (Agentes Inteligentes)
    - **Interface:** Streamlit
    - **Dados:** Pandas
    """)
    
    st.sidebar.markdown("### üîó Links √öteis")
    st.sidebar.markdown("""
    - [Google AI Studio](https://ai.google.dev/)
    - [Documenta√ß√£o LangChain (Python)](https://python.langchain.com/)
    - [Documenta√ß√£o Streamlit](https://docs.streamlit.io/)
    """)
    st.sidebar.markdown("---")
    st.sidebar.caption(f"Vers√£o da Aplica√ß√£o: 1.2.0 (Fix: Erro max_execution_time)")


if __name__ == "__main__":
    # √â crucial que o arquivo utils_google.py com as classes NotaFiscalValidator 
    # e a fun√ß√£o extract_zip_file esteja no mesmo diret√≥rio ou acess√≠vel no PYTHONPATH.
    # Se precisar de um dummy para testar a interface do Streamlit sem a l√≥gica completa:
    # try:
    #     from utils_google import NotaFiscalValidator, extract_zip_file
    # except ImportError:
    #     print("AVISO: utils_google.py n√£o encontrado. Usando implementa√ß√µes dummy para NotaFiscalValidator e extract_zip_file.")
    #     class NotaFiscalValidator:
    #         def identify_file_type(self, file_path_str): # Renomear para evitar conflito
    #             if "cabecalho" in str(file_path_str).lower(): return "cabecalho"
    #             if "itens" in str(file_path_str).lower(): return "itens"
    #             return "unknown"
    #     def extract_zip_file(uploaded_file_obj): # Renomear para evitar conflito
    #         import tempfile, zipfile, os
    #         temp_dir_obj = tempfile.mkdtemp() # Renomear para evitar conflito
    #         try:
    #             with zipfile.ZipFile(uploaded_file_obj, 'r') as zip_ref:
    #                 zip_ref.extractall(temp_dir_obj)
    #             return temp_dir_obj
    #         except Exception as e_dummy_zip:
    #             print(f"Erro no dummy extract_zip_file: {e_dummy_zip}")
    #             # Criar um diret√≥rio vazio para evitar que o resto falhe se o zip for inv√°lido no teste
    #             if not os.path.exists(temp_dir_obj): os.makedirs(temp_dir_obj)
    #             return temp_dir_obj
    #     # Atribuir globalmente para que o main() as encontre
    #     globals()['NotaFiscalValidator'] = NotaFiscalValidator
    #     globals()['extract_zip_file'] = extract_zip_file

    main()
