import streamlit as st
import pandas as pd
import zipfile
import os
import tempfile
from io import StringIO

# Tente este import alternativo:
try:
    from langchain_experimental.agents import create_csv_agent
    from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent 
except ImportError:
    from langchain.agents import create_csv_agent
    # Para vers√µes mais antigas de langchain, create_pandas_dataframe_agent pode estar em langchain.agents
    try:
        from langchain.agents import create_pandas_dataframe_agent
    except ImportError:
        # Se ainda n√£o encontrar, pode ser um sinal de que a vers√£o do langchain √© muito antiga
        # ou que o m√≥dulo espec√≠fico n√£o est√° instalado como esperado.
        # Por enquanto, vamos permitir que falhe mais tarde se n√£o for usado.
        pass


from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.agents.agent_types import AgentType
from utils_google import NotaFiscalValidator, extract_zip_file # Supondo que utils_google.py exista e funcione
import warnings
from dotenv import load_dotenv

load_dotenv()
warnings.filterwarnings("ignore")

class CSVAnalysisAgent:
    def __init__(self, google_api_key=None):
        """Inicializa o agente de an√°lise CSV com Google Gemini"""
        self.google_api_key = google_api_key
        self.agents = {}  # Armazena agentes para tipos de arquivo individuais (uso opcional)
        self.dataframes = {}  # Armazena dataframes carregados {file_type: df}
        self.file_info = {}  # Armazena informa√ß√µes sobre arquivos {file_type: {path, shape, columns}}
        self.cabecalho_file_type_name = "cabecalho" # Nome padr√£o para o tipo de arquivo de cabe√ßalho da nota fiscal

    def create_llm(self):
        """Cria uma inst√¢ncia do modelo Google Gemini"""
        if not self.google_api_key:
            # Tenta pegar da vari√°vel de ambiente se n√£o foi passada explicitamente
            self.google_api_key = os.getenv("GOOGLE_API_KEY")
            if not self.google_api_key:
                raise ValueError("Google API Key √© necess√°ria para usar o agente e n√£o foi encontrada.")
        
        try:
            llm = ChatGoogleGenerativeAI(
                model="gemini-1.5-flash", # ou "gemini-pro" se preferir/tiver acesso
                google_api_key=self.google_api_key,
                temperature=0.5, # Mantido conforme original
                convert_system_message_to_human=True,
            )
            return llm
        except Exception as e:
            st.error(f"Erro ao criar modelo Gemini: {str(e)}")
            return None
    
    def load_csv_data(self, file_path, file_type):
        """Carrega dados CSV."""
        try:
            # Tenta diferentes encodings se utf-8 falhar
            try:
                df = pd.read_csv(file_path, encoding='utf-8', sep=None, engine='python') # sep=None para autodetectar
            except UnicodeDecodeError:
                df = pd.read_csv(file_path, encoding='latin1', sep=None, engine='python')
            except Exception as e_read: # Captura outros erros de leitura
                st.error(f"Erro ao ler CSV {os.path.basename(file_path)} com encoding padr√£o: {e_read}. Tentando com delimitador ';'...")
                try:
                    df = pd.read_csv(file_path, encoding='utf-8', sep=';')
                except UnicodeDecodeError:
                    df = pd.read_csv(file_path, encoding='latin1', sep=';')
                except Exception as e_read_semi:
                     st.error(f"N√£o foi poss√≠vel ler o arquivo {os.path.basename(file_path)}: {e_read_semi}")
                     return False

            self.dataframes[file_type] = df
            self.file_info[file_type] = {
                'path': file_path, 
                'shape': df.shape,
                'columns': df.columns.tolist()
            }
            return True
            
        except Exception as e:
            st.error(f"Erro ao carregar arquivo {file_type} ({os.path.basename(file_path)}): {str(e)}")
            return False
    
    def create_general_agent(self):
        """Cria um agente geral que pode acessar todos os dataframes carregados."""
        try:
            if not self.dataframes:
                print("Erro: Nenhum dataframe carregado.")
                st.error("Nenhum dataframe carregado para criar o agente geral.")
                return None
            
            llm = self.create_llm()
            if llm is None:
                print("Erro: N√£o foi poss√≠vel criar LLM para o agente geral.")
                st.error("N√£o foi poss√≠vel criar o modelo de linguagem para o agente geral.")
                return None
            
            ordered_df_tuples = []
            # Usar as chaves de self.dataframes garante a ordem de inser√ß√£o (Python 3.7+)
            # Isso √© importante para a consist√™ncia de df_0, df_1, ...
            for file_type_key in self.dataframes.keys(): 
                df_instance = self.dataframes.get(file_type_key)
                # Validar se o dataframe √© uma inst√¢ncia de pd.DataFrame
                if isinstance(df_instance, pd.DataFrame):
                     ordered_df_tuples.append((file_type_key, df_instance))
                else:
                    print(f"Aviso: '{file_type_key}' n√£o √© um DataFrame v√°lido. Ser√° ignorado.")

            if not ordered_df_tuples:
                print("Erro: Nenhum dataframe v√°lido para o agente.")
                st.error("Nenhum dataframe v√°lido encontrado para criar o agente.")
                return None

            list_of_dfs = [df_tuple[1] for df_tuple in ordered_df_tuples]
            list_of_df_names = [df_tuple[0] for df_tuple in ordered_df_tuples]
            
            cabecalho_df_index = -1
            st.session_state.pop('cabecalho_df_variable_name', None) # Limpar antes de tentar definir
            try:
                cabecalho_df_index = list_of_df_names.index(self.cabecalho_file_type_name)
                st.session_state.cabecalho_df_variable_name = f"df_{cabecalho_df_index}"
                print(f"√çndice do DataFrame de cabe√ßalho ('{self.cabecalho_file_type_name}'): {cabecalho_df_index} (ser√° {st.session_state.cabecalho_df_variable_name})")
            except ValueError:
                print(f"Aviso: DataFrame do tipo '{self.cabecalho_file_type_name}' n√£o encontrado na lista: {list_of_df_names}")


            # Estrat√©gia Principal: create_pandas_dataframe_agent se houver DataFrames
            if list_of_dfs:
                try:
                    print(f"üîÑ Tentando criar agente pandas com {len(list_of_dfs)} dataframes: {list_of_df_names}")
                    
                    prefix_parts = [
                        "Voc√™ √© um agente de an√°lise de dados altamente competente, especializado em notas fiscais brasileiras.",
                        f"Voc√™ tem acesso a {len(list_of_dfs)} dataframes pandas nomeados df_0, df_1, ...:",
                    ]
                    for i, name in enumerate(list_of_df_names):
                        cols = self.dataframes[name].columns.to_list() if isinstance(self.dataframes.get(name), pd.DataFrame) else "Colunas n√£o dispon√≠veis"
                        prefix_parts.append(f"- df_{i}: (Tipo Original: '{name}'). Colunas: {cols}")

                    if cabecalho_df_index != -1:
                        prefix_parts.append(f"IMPORTANTE: O dataframe 'df_{cabecalho_df_index}' (tipo '{self.cabecalho_file_type_name}') √© o principal para an√°lise de totais por fornecedor, pois cont√©m os cabe√ßalhos das notas fiscais. Use-o para essa finalidade.")
                    else:
                        prefix_parts.append(f"IMPORTANTE: O dataframe do tipo '{self.cabecalho_file_type_name}' √© o principal para an√°lise de totais por fornecedor. Identifique qual df_X (df_0, df_1, etc.) corresponde a este tipo e use-o para essa finalidade.")
                    
                    prefix_message = "\n".join(prefix_parts)
                    
                    # create_pandas_dataframe_agent espera um √∫nico DF ou uma lista de DFs.
                    input_for_agent = list_of_dfs[0] if len(list_of_dfs) == 1 else list_of_dfs

                    general_agent = create_pandas_dataframe_agent(
                        llm,
                        input_for_agent, 
                        verbose=True,
                        agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, # Ou OPENAI_FUNCTIONS se preferir
                        allow_dangerous_code=True,
                        handle_parsing_errors=" –∞–≥–µ–Ω—Ç–æ–º –±—É–¥–µ—Ç –ø—Ä–µ–¥–ø—Ä–∏–Ω—è—Ç–∞ –ø–æ–ø—ã—Ç–∫–∞ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –æ—à–∏–±–∫—É —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ ", # String para robustez
                        prefix=prefix_message,
                        max_iterations=15, 
                        early_stopping_method="generate",
                        agent_executor_kwargs={
                            "handle_parsing_errors": True, 
                            "max_execution_time": 600 # Segundos
                        }
                    )
                    print(f"‚úÖ Agente pandas criado com sucesso para {len(list_of_dfs)} dataframes.")
                    st.session_state.current_agent_type = "pandas_multi"
                    return general_agent
                
                except Exception as e_pandas:
                    print(f"‚ùå Erro ao criar agente pandas: {str(e_pandas)}")
                    st.warning(f"Falha ao criar agente pandas: {e_pandas}. Tentando fallback para agente CSV (se houver caminhos v√°lidos)...")
                    st.session_state.pop('cabecalho_df_variable_name', None) 

            # Fallback para create_csv_agent (usando caminhos)
            valid_paths = [self.file_info.get(ft, {}).get('path') for ft in list_of_df_names 
                           if self.file_info.get(ft, {}).get('path') and os.path.exists(self.file_info.get(ft, {}).get('path'))]

            if not valid_paths:
                st.error("Nenhum caminho de arquivo CSV v√°lido encontrado para o agente de fallback.")
                return None
            
            agent_path_input = valid_paths[0] if len(valid_paths) == 1 else valid_paths
            print(f"Tentando criar agente CSV com {len(valid_paths)} arquivos: {agent_path_input}")
            try:
                general_agent = create_csv_agent(
                    llm,
                    agent_path_input, 
                    verbose=True,
                    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
                    allow_dangerous_code=True,
                    handle_parsing_errors=True,
                    max_iterations=30, # Conforme original
                    early_stopping_method="generate",
                    agent_executor_kwargs={
                        "handle_parsing_errors": True,
                        "max_execution_time": 600
                    }
                )
                print(f"‚úÖ Agente CSV (path-based) criado com sucesso para {len(valid_paths)} arquivos.")
                st.session_state.current_agent_type = "csv_multi_path" if isinstance(agent_path_input, list) else "csv_single_path"
                
                # Se for um √∫nico arquivo CSV, verificar se √© o de cabe√ßalho
                if st.session_state.current_agent_type == "csv_single_path":
                    first_file_type_path = None
                    for f_type, info in self.file_info.items():
                        if info['path'] == agent_path_input: # agent_path_input √© string aqui
                            first_file_type_path = f_type
                            break
                    if first_file_type_path == self.cabecalho_file_type_name:
                        st.session_state.cabecalho_df_variable_name = "df" # Agente CSV √∫nico refere-se a ele como 'df'
                    else:
                        st.session_state.pop('cabecalho_df_variable_name', None)
                return general_agent
            except Exception as e_csv:
                print(f"‚ùå Erro ao criar agente CSV (path-based): {str(e_csv)}")
                st.error(f"Falha cr√≠tica ao criar qualquer agente: {e_csv}")
                return None

        except Exception as e_general:
            print(f"‚ùå Erro geral ao criar agente: {str(e_general)}")
            st.error(f"Erro geral ao criar agente de an√°lise: {str(e_general)}")
            return None

    def query(self, question):
        """Executa uma consulta usando o agente geral (recriado a cada consulta para refletir dados atuais)."""
        try:
            agent = self.create_general_agent() 
            if agent is None:
                return "Erro: N√£o foi poss√≠vel inicializar o agente de an√°lise para a consulta."

            target_df_variable_for_prompt = "df" 
            identified_cabecalho_type_for_prompt = f"'{self.cabecalho_file_type_name}'"
            
            current_agent_context = st.session_state.get("current_agent_type")
            cabecalho_df_var_from_session = st.session_state.get("cabecalho_df_variable_name")

            if current_agent_context == "pandas_multi" and cabecalho_df_var_from_session:
                target_df_variable_for_prompt = cabecalho_df_var_from_session
                print(f"Query: Usando '{target_df_variable_for_prompt}' para c√≥digo pandas (agente pandas_multi).")
            elif current_agent_context in ["csv_single_path", "csv_single_fallback"] and cabecalho_df_var_from_session == "df":
                target_df_variable_for_prompt = "df"
                print(f"Query: Usando 'df' para c√≥digo pandas (agente CSV √∫nico sobre cabe√ßalho).")
            elif current_agent_context == "csv_multi_path":
                print(f"Query: Agente csv_multi_path. O agente precisar√° identificar e carregar o CSV '{self.cabecalho_file_type_name}'.")
                identified_cabecalho_type_for_prompt = f"o arquivo CSV correspondente a '{self.cabecalho_file_type_name}'"
                # target_df_variable_for_prompt continua "df", agente deve carregar o cabecalho nele.
            else: # Outros casos ou se cabecalho_df_variable_name n√£o estiver definido
                 print(f"AVISO: Contexto do agente √© '{current_agent_context}'. O nome da vari√°vel do dataframe de cabe√ßalho n√£o foi determinado explicitamente para o prompt. O Agente precisar√° inferir qual dataframe usar para '{self.cabecalho_file_type_name}'.")
                 identified_cabecalho_type_for_prompt = f"o dataframe/arquivo correspondente a '{self.cabecalho_file_type_name}'"


            coluna_fornecedor = 'RAZ√ÉO SOCIAL EMITENTE'
            coluna_valor = 'VALOR NOTA FISCAL'

            pandas_code_block = f"""
```python
import pandas as pd

# O agente deve usar o dataframe que representa os dados de {identified_cabecalho_type_for_prompt}.
# No contexto de um agente pandas_multi, esta vari√°vel j√° foi definida (ex: df_0, df_1), use '{target_df_variable_for_prompt}'.
# No contexto de um agente CSV (path-based), carregue o arquivo CSV de cabe√ßalho em um dataframe (ex: {target_df_variable_for_prompt} = pd.read_csv('path_do_cabecalho.csv')).
# O c√≥digo abaixo assume que '{target_df_variable_for_prompt}' √© o dataframe de cabe√ßalho j√° dispon√≠vel ou carregado.

print(f"Iniciando an√°lise no dataframe '{target_df_variable_for_prompt}' (espera-se que seja {identified_cabecalho_type_for_prompt}).")
try:
    # Valida√ß√£o crucial: O agente precisa garantir que '{target_df_variable_for_prompt}' realmente existe e √© o dataframe correto.
    # Esta √© uma responsabilidade do agente se ele precisar carregar o CSV.
    # Exemplo de como o agente poderia fazer isso (n√£o faz parte do c√≥digo a ser copiado literalmente, mas da l√≥gica do agente):
    # if not isinstance({target_df_variable_for_prompt}, pd.DataFrame):
    #    path_cabecalho = find_path_for_file_type('{self.cabecalho_file_type_name}') # L√≥gica interna do agente
    #    {target_df_variable_for_prompt} = pd.read_csv(path_cabecalho)

    print("Colunas dispon√≠veis em '{target_df_variable_for_prompt}':", {target_df_variable_for_prompt}.columns.tolist())
    print("\\nPrimeiras 5 linhas de '{target_df_variable_for_prompt}':\\n", {target_df_variable_for_prompt}.head())

    if '{coluna_valor}' not in {target_df_variable_for_prompt}.columns:
        raise ValueError("Coluna de valor '{coluna_valor}' N√ÉO ENCONTRADA no dataframe '{target_df_variable_for_prompt}'. Verifique o nome da coluna.")
    if '{coluna_fornecedor}' not in {target_df_variable_for_prompt}.columns:
        raise ValueError("Coluna de fornecedor '{coluna_fornecedor}' N√ÉO ENCONTRADA no dataframe '{target_df_variable_for_prompt}'. Verifique o nome da coluna.")

    # Garantir que a coluna de valor seja string antes de aplicar m√©todos str.str.replace
    df_analysis = {target_df_variable_for_prompt}.copy() # Trabalhar com uma c√≥pia para evitar SettingWithCopyWarning
    if df_analysis['{coluna_valor}'].dtype == 'object':
        df_analysis.loc[:, '{coluna_valor}'] = df_analysis['{coluna_valor}'].astype(str).str.replace('.', '', regex=False).str.replace(',', '.', regex=False)
    
    df_analysis.loc[:, '{coluna_valor}'] = pd.to_numeric(df_analysis['{coluna_valor}'], errors='coerce')
    
    print(f"Valores ausentes em '{coluna_valor}' ap√≥s convers√£o num√©rica: {{df_analysis['{coluna_valor}'].isnull().sum()}}")
    df_analysis.dropna(subset=['{coluna_valor}', '{coluna_fornecedor}'], inplace=True) 

    if df_analysis.empty:
        raise ValueError("O DataFrame ficou vazio ap√≥s limpeza de dados (NaN em valor ou fornecedor). N√£o √© poss√≠vel prosseguir.")

    print(f"Agrupando por '{coluna_fornecedor}' e somando '{coluna_valor}'. Total de linhas para agrupar: {{len(df_analysis)}}")
    resultado = df_analysis.groupby('{coluna_fornecedor}')['{coluna_valor}'].sum()

    resultado_ordenado = resultado.sort_values(ascending=False)

    if not resultado_ordenado.empty:
        maior_fornecedor = resultado_ordenado.index[0]
        maior_valor = resultado_ordenado.iloc[0]
        
        # Formata√ß√£o para o padr√£o monet√°rio brasileiro R$ xxx.xxx,xx
        maior_valor_formatado = f"R$ {{maior_valor:_.2f}}".replace('.', '#').replace(',', '.').replace('#', ',')
        # Se o resultado for, por exemplo, R$ 1,292,418.75 (locale US), queremos R$ 1.292.418,75
        # A formata√ß√£o :_.2f -> 1_292_418.75. replace . com , (1_292_418,75). replace _ com . (1.292.418,75)
        # Ajuste manual se necess√°rio:
        # temp_val = f"{{maior_valor:,.2f}}" # Ex: 1,292,418.75
        # maior_valor_formatado = "R$ " + temp_val.replace(",", "X").replace(".", ",").replace("X", ".")


        print(f"\\nFORNECEDOR COM MAIOR MONTANTE TOTAL CONSOLIDADO: {{maior_fornecedor}}")
        print(f"VALOR TOTAL CONSOLIDADO: {{maior_valor_formatado}}")

        print("\\nTOP 5 FORNECEDORES (VALORES TOTAIS CONSOLIDADOS):")
        for i, (fornecedor, valor) in enumerate(resultado_ordenado.head().items()):
            valor_fmt = f"R$ {{valor:_.2f}}".replace('.', '#').replace(',', '.').replace('#', ',')
            # temp_item_val = f"{{valor:,.2f}}"
            # valor_fmt = "R$ " + temp_item_val.replace(",", "X").replace(".", ",").replace("X", ".")
            print(f"{{i+1}}. {{fornecedor}}: {{valor_fmt}}")
    else:
        print("N√£o foi poss√≠vel calcular os resultados. O dataframe agrupado est√° vazio (nenhum fornecedor encontrado ou todos os valores eram inv√°lidos).")
        
except Exception as e_pandas_code:
    print(f"ERRO AO EXECUTAR O C√ìDIGO PANDAS DENTRO DO AGENTE: {{str(e_pandas_code)}}")
    print("Verifique se o dataframe '{target_df_variable_for_prompt}' foi carregado/identificado corretamente pelo agente e se as colunas '{coluna_fornecedor}' e '{coluna_valor}' existem e s√£o adequadas para a an√°lise.")

```"""
            available_data_message = f"DADOS DISPON√çVEIS NO SISTEMA: {', '.join(self.dataframes.keys()) if self.dataframes else 'Nenhum dataframe carregado diretamente no sistema Python. Agente pode precisar carregar de caminhos.'}"
            
            guidance_on_df_usage = (
                f"Voc√™ DEVE USAR o dataframe '{target_df_variable_for_prompt}' (que o sistema identificou como correspondendo a {identified_cabecalho_type_for_prompt}) para esta an√°lise, pois ele cont√©m os cabe√ßalhos das notas fiscais."
                if cabecalho_df_var_from_session or (current_agent_context in ["csv_single_path", "csv_single_fallback"] and identified_cabecalho_type_for_prompt == f"'{self.cabecalho_file_type_name}'")
                else f"Voc√™ deve priorizar o uso dos dados do arquivo/dataframe do tipo '{self.cabecalho_file_type_name}'. Se m√∫ltiplos dataframes (df_0, df_1, ...) ou arquivos CSV estiverem dispon√≠veis, identifique qual deles corresponde a '{self.cabecalho_file_type_name}' (o prefixo do agente, se aplic√°vel, j√° deu essa informa√ß√£o) e use-o para a an√°lise de totais por fornecedor. O c√≥digo pandas fornecido usa '{target_df_variable_for_prompt}' como placeholder para este dataframe de cabe√ßalho."
            )

            enhanced_question = f"""
OBJETIVO PRINCIPAL: Identificar o fornecedor com o MAIOR VALOR TOTAL CONSOLIDADO de notas fiscais.

{available_data_message}
{guidance_on_df_usage}

PERGUNTA ORIGINAL DO USU√ÅRIO: {question}

‚ö†Ô∏è ATEN√á√ÉO CR√çTICA E OBRIGAT√ìRIA - LEIA COM ATEN√á√ÉO M√ÅXIMA:
Para determinar o "maior" fornecedor, N√ÉO olhe para valores de notas fiscais individuais.
VOC√ä DEVE, OBRIGATORIAMENTE, SEGUIR ESTES PASSOS:
1. AGRUPAR OS DADOS POR FORNECEDOR (usando a coluna '{coluna_fornecedor}').
2. SOMAR (usar a fun√ß√£o `.sum()`) TODOS os valores de notas fiscais (da coluna '{coluna_valor}') para CADA fornecedor individualmente.
Isto resultar√° no MONTANTE TOTAL CONSOLIDADO para cada fornecedor. A an√°lise √© sobre este valor consolidado.

METODOLOGIA OBRIGAT√ìRIA PARA RESPONDER CORRETAMENTE:
1. IDENTIFICA√á√ÉO DO DATAFRAME: Certifique-se de que est√° usando o DataFrame correto que cont√©m os dados de cabe√ßalho das notas fiscais (referido como '{target_df_variable_for_prompt}' no c√≥digo abaixo, que deve corresponder a {identified_cabecalho_type_for_prompt}). Se for um agente baseado em CSV, carregue o arquivo CSV de cabe√ßalho apropriado.
2. COLUNAS CRUCIAIS: As colunas s√£o '{coluna_fornecedor}' (nome do fornecedor) e '{coluna_valor}' (valor da nota).
3. CONVERS√ÉO DE VALOR (ESSENCIAL!): ANTES DE QUALQUER C√ÅLCULO, a coluna '{coluna_valor}' DEVE ser convertida para um tipo num√©rico (float). √â comum ela estar como string (ex: "1.234,56" ou "6712.16"). O c√≥digo pandas fornecido abaixo j√° inclui a l√≥gica para converter strings no formato "X.XXX,YY" para o num√©rico XXXX.YY (removendo '.' e trocando ',' por '.'). Valores que n√£o puderem ser convertidos devem se tornar NaN e as linhas correspondentes (ou pelo menos com valor NaN) devem ser removidas antes da agrega√ß√£o.
4. AGRUPAMENTO: Agrupe os dados pela coluna '{coluna_fornecedor}'.
5. SOMA CONSOLIDADA: SOME (use `.sum()`) TODOS os valores da coluna '{coluna_valor}' para CADA fornecedor.
6. ORDENA√á√ÉO: Ordene os fornecedores de forma descendente pelo valor total CONSOLIDADO.
7. RESULTADO: O fornecedor no topo desta lista ordenada √© a resposta.

‚ùå EXEMPLO CLAR√çSSIMO DO ERRO GRAVE A SER EVITADO (N√ÉO FA√áA ISSO DE FORMA ALGUMA):
- ERRADO: Olhar uma √∫nica nota fiscal da "EDITORA FTD S.A." no valor de R$ 6.712,16 e, por ser um valor alto individual, concluir que este √© o resultado. ISTO EST√Å COMPLETAMENTE ERRADO.
- CORRETO: Somar TODAS as notas fiscais da "EDITORA FTD S.A." (ex: o total consolidado dela pode ser R$ 292.486,11) e comparar esta SOMA CONSOLIDADA com a SOMA CONSOLIDADA de TODAS as notas de outros fornecedores.
- EXEMPLO DE RESULTADO CORRETO ESPERADO (valores meramente ilustrativos para o conceito): "O fornecedor CHEMYUNION LTDA teve o maior montante total consolidado, com R$ 1.292.418,75."

‚û°Ô∏è C√ìDIGO PANDAS OBRIGAT√ìRIO - Execute a l√≥gica descrita abaixo. Adapte '{target_df_variable_for_prompt}' para o nome correto do dataframe de cabe√ßalho se ele for diferente (ex: df_0, df_1, ou o resultado de pd.read_csv('caminho/para/cabecalho.csv')). O agente √© respons√°vel por garantir que '{target_df_variable_for_prompt}' no c√≥digo abaixo seja o dataframe correto de cabe√ßalho.
{pandas_code_block}

FORMATO DA RESPOSTA FINAL (em portugu√™s brasileiro):
- "O fornecedor com o maior montante total consolidado √©: [Nome do Fornecedor]."
- "Valor total consolidado: [Valor Formatado no padr√£o R$ xxx.xxx,xx]."
- Inclua uma se√ß√£o "C√≥digo Pandas Chave Executado e Observa√ß√µes:" onde voc√™ mostra o trecho principal do c√≥digo pandas que voc√™ efetivamente usou para o c√°lculo (ou um resumo claro da sua execu√ß√£o, qual dataframe foi usado, e a forma como '{coluna_valor}' foi tratada e agregada) e quaisquer observa√ß√µes importantes (ex: n√∫mero de linhas analisadas, tratamento de erros).
- "Top 5 fornecedores por valor total consolidado:"
  1. [Fornecedor A]: [Valor A Formatado R$ xxx.xxx,xx]
  2. [Fornecedor B]: [Valor B Formatado R$ xxx.xxx,xx]
  ... e assim por diante.

INSTRU√á√ïES CR√çTICAS ADICIONAIS (RELEIA ANTES DE RESPONDER):
1. FOCO ABSOLUTO NA AGREGA√á√ÉO: `groupby('{coluna_fornecedor}')['{coluna_valor}'].sum()`. √â a chave.
2. VALIDA√á√ÉO DE DADOS: SEMPRE verifique `df.info()`, `df.head()` do dataframe de cabe√ßalho. A convers√£o da coluna de valor √© a etapa mais cr√≠tica antes da soma.
3. FORMATA√á√ÉO MONET√ÅRIA BRASILEIRA: Apresente valores finais como R$ XX.XXX.XXX,XX (ponto como separador de milhar, v√≠rgula para decimal). O c√≥digo exemplo tenta fazer isso com `R$ {{valor:_.2f}}".replace('.', '#').replace(',', '.').replace('#', ',')`. Certifique-se que a sa√≠da esteja correta.
4. RESPONDA EM PORTUGU√äS BRASILEIRO.
5. N√ÉO INVENTE DADOS. Use apenas os dados dos arquivos fornecidos.
6. SEJA EXPL√çCITO sobre qual arquivo/dataframe foi usado para a an√°lise principal e como chegou ao resultado.
7. SE O DATAFRAME DE CABE√áALHO ESTIVER VAZIO ou n√£o contiver as colunas necess√°rias, informe isso claramente.
"""
            
            response = agent.invoke({"input": enhanced_question})
            
            result = ""
            if isinstance(response, dict):
                # Tentar extrair de 'output', depois 'result', depois outros comuns
                result = response.get("output", response.get("result", response.get("answer", "")))
                if not result and response: # Se ainda vazio, pegar o primeiro valor do dict se for string
                    for val in response.values():
                        if isinstance(val, str):
                            result = val
                            break
            else:
                result = str(response)
            
            if result and result.strip():
                result_str = result.strip()
                # Limpeza mais agressiva de logs de pensamento do agente
                lines = result_str.split('\n')
                clean_lines = []
                in_tool_code_block = False # Para n√£o limpar c√≥digo python √∫til
                for line in lines:
                    # Verificar se √© uma linha de c√≥digo python dentro de ```python ... ```
                    if line.strip().startswith("```python"):
                        in_tool_code_block = True
                        clean_lines.append(line)
                        continue
                    if line.strip().startswith("```") and in_tool_code_block:
                        in_tool_code_block = False
                        clean_lines.append(line)
                        continue
                    
                    if in_tool_code_block: # Manter linhas dentro do bloco de c√≥digo
                         clean_lines.append(line)
                         continue

                    # Termos a serem removidos se n√£o estiverem em bloco de c√≥digo
                    debug_terms_to_remove = [
                        '> entering new agentexecutor chain', 
                        '> entering new llmchain object',
                        'invoking agent with', 'invoking llm', 
                        'tool execution result',
                        'thought:', 'action:', 'action input:', 'observation:', # Comuns em ReAct
                        'final answer:', # O prompt j√° pede formato espec√≠fico
                        # Cuidado com "print(" se o agente mostrar c√≥digo √∫til com print
                    ]
                    # Remover apenas se a linha INTEIRA for um desses termos ou come√ßar com eles de forma gen√©rica
                    # √â melhor ser conservador para n√£o remover partes da resposta real.
                    # A l√≥gica de limpeza pode ser complexa; o ideal √© o agente responder de forma limpa.
                    # Por agora, uma limpeza leve.
                    temp_line = line.lower()
                    should_remove = False
                    for term in debug_terms_to_remove:
                        if temp_line.startswith(term):
                            should_remove = True
                            break
                    if not should_remove:
                         # Remover linhas que s√£o apenas "Okay." ou "Got it."
                        if temp_line.strip() not in ["okay.", "got it.", "sure."]:
                            clean_lines.append(line)

                result_str = '\n'.join(clean_lines).strip()

                # Garantir tradu√ß√µes se o LLM escorregar (o prompt √© forte, mas por via das d√∫vidas)
                result_str = result_str.replace('The supplier with the highest total consolidated amount is:', 'O fornecedor com o maior montante total consolidado √©:')
                result_str = result_str.replace('Total consolidated value:', 'Valor total consolidado:')
                result_str = result_str.replace('Top 5 suppliers by total consolidated value:', 'Top 5 fornecedores por valor total consolidado:')
                result_str = result_str.replace('Key Pandas Code Executed and Observations:', 'C√≥digo Pandas Chave Executado e Observa√ß√µes:')
                result_str = result_str.replace('FINAL ANSWER:', '') # Remover se aparecer

                return result_str
            else:
                return "O agente n√£o conseguiu processar a consulta ou n√£o retornou uma resposta estruturada. Verifique os logs do console para depura√ß√£o."
                
        except Exception as e:
            st.error(f"Erro cr√≠tico ao processar consulta na camada de query: {str(e)}")
            import traceback
            traceback.print_exc() # Logar no console do servidor
            return f"Erro cr√≠tico ao processar sua consulta: {str(e)}. Por favor, verifique os logs do console do servidor ou tente novamente."

    def get_data_summary(self):
        """Retorna um resumo dos dados carregados."""
        summary = {}
        if not self.dataframes:
            return {"message": "Nenhum dado carregado ainda."}
        for file_type, df_instance in self.dataframes.items():
            if isinstance(df_instance, pd.DataFrame):
                summary[file_type] = {
                    'linhas': len(df_instance),
                    'colunas': len(df_instance.columns),
                    'colunas_lista': df_instance.columns.tolist(),
                    'tipos': {col: str(dtype) for col, dtype in df_instance.dtypes.to_dict().items()},
                    'primeiras_linhas': df_instance.head().to_dict('records')
                }
            else:
                summary[file_type] = {"error": f"'{file_type}' n√£o √© um DataFrame v√°lido ou n√£o foi carregado."}
        return summary

def main():
    st.set_page_config(
        page_title="Agente de An√°lise de Notas Fiscais - Google Gemini",
        page_icon="üìä",
        layout="wide"
    )
    
    st.title("ü§ñ Agente Inteligente para An√°lise de Notas Fiscais")
    st.markdown("### Utilizando Google Gemini API & LangChain")
    st.markdown("---")
    
    # GOOGLE_API_KEY √© configurada no `create_llm` se n√£o passada, lendo de .env
    # Apenas garantir que o .env exista ou a chave seja configurada no ambiente
    if 'csv_agent_instance' not in st.session_state:
        try:
            st.session_state.csv_agent_instance = CSVAnalysisAgent() # API key ser√° lida em create_llm
        except ValueError as e: # Captura erro se API key n√£o for encontrada em create_llm
            st.error(f"Erro de inicializa√ß√£o: {e}")
            return
    
    csv_analyzer = st.session_state.csv_agent_instance

    st.header("üìÅ 1. Upload do Arquivo ZIP com CSVs")
    uploaded_file = st.file_uploader(
        "Fa√ßa upload do arquivo ZIP contendo os CSVs de notas fiscais (ex: cabecalho.csv, itens.csv)",
        type=['zip'],
        help=f"O ZIP deve conter arquivos CSV. Um deles deve ser o de '{csv_analyzer.cabecalho_file_type_name}' das notas fiscais para a an√°lise principal."
    )
    
    if uploaded_file is not None:
        with st.spinner("Processando arquivo ZIP e carregando dados..."):
            csv_analyzer.dataframes = {}
            csv_analyzer.file_info = {}
            st.session_state.pop('cabecalho_df_variable_name', None) 
            st.session_state.pop('current_agent_type', None)

            try:
                temp_dir = extract_zip_file(uploaded_file) 
            except Exception as e_zip:
                st.error(f"Falha ao extrair o arquivo ZIP: {e_zip}")
                temp_dir = None
            
            if temp_dir:
                st.success(f"‚úÖ Arquivo ZIP extra√≠do com sucesso para o diret√≥rio tempor√°rio.")
                
                try:
                    validator = NotaFiscalValidator() 
                except Exception as e_val_init:
                     st.error(f"Erro ao inicializar NotaFiscalValidator: {e_val_init}. Assegure-se que 'utils_google.py' est√° correto.")
                     return

                csv_files_found = [f for f in os.listdir(temp_dir) if f.lower().endswith('.csv')]
                
                if not csv_files_found:
                    st.warning("Nenhum arquivo CSV encontrado no ZIP.")
                else:
                    loaded_files_summary = []
                    for csv_file_name in csv_files_found:
                        file_full_path = os.path.join(temp_dir, csv_file_name)
                        try:
                            file_type_identified = validator.identify_file_type(file_full_path) 
                        except Exception as e_ident:
                            st.warning(f"Erro ao identificar tipo do arquivo {csv_file_name} com NotaFiscalValidator: {e_ident}. Usando nome do arquivo como tipo.")
                            file_type_identified = os.path.splitext(csv_file_name)[0].lower()

                        if file_type_identified == "unknown": # Validator pode retornar "unknown"
                            file_type_identified = f"tipo_desconhecido_{os.path.splitext(csv_file_name)[0]}"

                        success = csv_analyzer.load_csv_data(file_full_path, file_type_identified)
                        if success:
                            loaded_files_summary.append(f"{csv_file_name} (tipo: '{file_type_identified}')")
                
                    if loaded_files_summary:
                        st.success(f"‚úÖ Arquivos CSV carregados: {', '.join(loaded_files_summary)}")
                        
                        if csv_analyzer.cabecalho_file_type_name not in csv_analyzer.dataframes:
                            st.error(f"‚ÄºÔ∏è ATEN√á√ÉO: O arquivo/tipo de dados '{csv_analyzer.cabecalho_file_type_name}' √© ESSENCIAL para a an√°lise principal de totais por fornecedor e N√ÉO FOI ENCONTRADO ou identificado corretamente. Verifique o conte√∫do do ZIP e a l√≥gica de identifica√ß√£o em 'utils_google.py'. A an√°lise principal pode falhar.")
                        else:
                            st.info(f"üëç Arquivo/tipo de dados '{csv_analyzer.cabecalho_file_type_name}' carregado. Pronto para an√°lise de totais por fornecedor.")

                        with st.expander("üìä Resumo dos Dados Carregados"):
                            summary = csv_analyzer.get_data_summary()
                            for file_type_sum, info_sum in summary.items():
                                st.subheader(f"üìÑ {file_type_sum.replace('_', ' ').title()}")
                                if "error" in info_sum:
                                    st.error(info_sum["error"])
                                    continue
                                col1, col2 = st.columns(2)
                                with col1:
                                    st.metric("Linhas", info_sum.get('linhas', 'N/A'))
                                with col2:
                                    st.metric("Colunas", info_sum.get('colunas', 'N/A'))
                                st.write("**Nomes das Colunas:**")
                                st.caption(", ".join(info_sum.get('colunas_lista', [])))
                                
                                if st.checkbox(f"Ver primeiras 5 linhas - {file_type_sum}", key=f"show_head_{file_type_sum}"):
                                    df_preview = pd.DataFrame(info_sum.get('primeiras_linhas', []))
                                    st.dataframe(df_preview, use_container_width=True)
                    else:
                        st.error("Nenhum arquivo CSV p√¥de ser carregado do ZIP ou todos falharam.")
            #uploaded_file = None # Resetar para permitir novo upload se necess√°rio (Streamlit geralmente faz isso)

    if hasattr(csv_analyzer, 'dataframes') and csv_analyzer.dataframes:
        st.header("üîç 2. Fa√ßa sua Pergunta ao Agente")
        
        with st.expander("üí° Exemplos de Perguntas (foco na an√°lise de totais consolidados do 'cabecalho')"):
            st.markdown(f"""
            - Qual √© o fornecedor que teve maior montante recebido (total consolidado)? (Pergunta principal)
            - Quais s√£o os 5 fornecedores com maior valor total consolidado?
            - Qual a soma total de 'VALOR NOTA FISCAL' no arquivo '{csv_analyzer.cabecalho_file_type_name}'?
            - Quantas notas fiscais (linhas) existem no arquivo '{csv_analyzer.cabecalho_file_type_name}'?
            - Descreva as colunas do arquivo '{csv_analyzer.cabecalho_file_type_name}'.
            """)
        
        default_question = "Qual √© o fornecedor que teve maior montante recebido (total consolidado)?"
        user_question = st.text_area(
            "Sua pergunta:",
            placeholder=default_question,
            height=100,
            key="user_question_input",
            value=st.session_state.get("user_question_input_memory", default_question) # Manter a √∫ltima pergunta
        )
        st.session_state.user_question_input_memory = user_question # Salvar para a pr√≥xima vez
        
        if st.button("üöÄ Executar Consulta", type="primary", key="run_query_button"):
            if not user_question.strip():
                st.warning("‚ö†Ô∏è Por favor, digite uma pergunta.")
            elif csv_analyzer.cabecalho_file_type_name not in csv_analyzer.dataframes:
                 st.error(f"‚ÄºÔ∏è N√£o √© poss√≠vel executar a consulta principal. O arquivo/tipo de dados '{csv_analyzer.cabecalho_file_type_name}' n√£o foi carregado. Fa√ßa o upload de um ZIP contendo-o.")
            else:
                with st.spinner("üß† O Agente Gemini est√° processando sua consulta... Por favor, aguarde."):
                    response_from_agent = csv_analyzer.query(user_question) 
                    
                    st.markdown("### üìã Resposta do Agente:")
                    if response_from_agent and isinstance(response_from_agent, str) and response_from_agent.strip():
                        st.markdown(response_from_agent)
                    elif not response_from_agent:
                         st.error("O agente n√£o retornou uma resposta ou a resposta foi vazia.")
                    else: 
                        st.error(f"Resposta inesperada do agente (tipo: {type(response_from_agent)}): {response_from_agent}")
                    
                    # Para depura√ß√£o, mostrar o tipo de agente usado
                    if st.session_state.get("current_agent_type"):
                        st.caption(f"Debug: Agente usado: {st.session_state.current_agent_type}, Var. Cabe√ßalho no prompt: {st.session_state.get('cabecalho_df_variable_name', 'N/A')}")

    else:
        st.info("Fa√ßa o upload de um arquivo ZIP na se√ß√£o acima para come√ßar a an√°lise.")

    st.sidebar.markdown("---")
    st.sidebar.markdown("### üìö Sobre a Aplica√ß√£o")
    st.sidebar.markdown("""
    Esta aplica√ß√£o demonstra o uso de Intelig√™ncia Artificial Generativa para analisar dados de notas fiscais em formato CSV.
    - **IA:** Google Gemini API (Modelo gemini-1.5-flash)
    - **Orquestra√ß√£o:** LangChain (Agentes Inteligentes)
    - **Interface:** Streamlit
    - **Dados:** Pandas
    """)
    
    st.sidebar.markdown("### üîó Links √öteis")
    st.sidebar.markdown("""
    - [Google AI Studio](https://ai.google.dev/)
    - [Documenta√ß√£o LangChain (Python)](https://python.langchain.com/)
    - [Documenta√ß√£o Streamlit](https://docs.streamlit.io/)
    """)
    st.sidebar.markdown("---")
    st.sidebar.caption(f"Vers√£o da Aplica√ß√£o: 1.1.0")


if __name__ == "__main__":
    # Assegurar que utils_google.py exista ou fornecer implementa√ß√µes dummy se necess√°rio para teste local
    # Exemplo de utils_google.py dummy:
    # class NotaFiscalValidator:
    #     def identify_file_type(self, file_path):
    #         if "cabecalho" in file_path.lower():
    #             return "cabecalho"
    #         if "itens" in file_path.lower():
    #             return "itens"
    #         return "unknown"
    # def extract_zip_file(uploaded_file):
    #     import tempfile, zipfile, os
    #     temp_dir = tempfile.mkdtemp()
    #     with zipfile.ZipFile(uploaded_file, 'r') as zip_ref:
    #         zip_ref.extractall(temp_dir)
    #     return temp_dir
    main()
